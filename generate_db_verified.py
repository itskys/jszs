import os
import hashlib
import json
import re
try:
    from docx import Document
except ImportError:
    print("âŒ è¯·å®‰è£…ä¾èµ–: pip install python-docx")
    exit()

# ================= é…ç½® =================
INPUT_DIR = './2025tk'
OUTPUT_JS = 'questions_data.js'
REPORT_FILE = 'extraction_report.txt'

# ================= æ­£åˆ™ =================
RE_OPTION = re.compile(r'^\s*\(?([A-F])\)?[ã€\.ï¼\s]\s*(.*)', re.IGNORECASE)
RE_ANSWER = re.compile(r'^\s*(?:æ­£ç¡®)?ç­”æ¡ˆ[:ï¼š]\s*(.*)', re.IGNORECASE)
RE_ANALYSIS = re.compile(r'^\s*(?:ç­”æ¡ˆ)?è§£æ[:ï¼š]\s*(.*)', re.IGNORECASE)
RE_CHAPTER_IN_ANA = re.compile(r'(?:<br>|[\s\n])*(æ‰€åœ¨ç« èŠ‚[:ï¼š].*)')

# è®°å½•æ—¥å¿—çš„ç¼“å†²åŒº
log_buffer = []

def log(msg):
    print(msg)
    log_buffer.append(msg)

def normalize_text(text):
    if not text: return ""
    text = re.sub(r'<[^>]+>', '', text) 
    return re.sub(r'[^\w\u4e00-\u9fa5]', '', text).strip()

def clean_answer(ans):
    if not ans: return ""
    ans = ans.strip().upper()
    if ans in ['âˆš', 'T', 'Y', 'TRUE', 'æ­£ç¡®']: return 'æ­£ç¡®'
    if ans in ['Ã—', 'F', 'N', 'FALSE', 'é”™è¯¯']: return 'é”™è¯¯'
    valid = [c for c in ans if c in "ABCDEF"]
    return "".join(valid)

def infer_type(filename):
    if 'å¤šé€‰' in filename: return 'multi'
    if 'å•é€‰' in filename: return 'single'
    if 'åˆ¤æ–­' in filename: return 'tf'
    return 'unknown'

def parse_docx(file_path):
    fname = os.path.basename(file_path)
    try:
        doc = Document(file_path)
    except:
        log(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {fname}")
        return []

    questions = []
    curr = None
    collecting_txt = False
    
    # ç»Ÿè®¡è¯¥æ–‡ä»¶çš„è¡Œæ•°
    total_paragraphs = len(doc.paragraphs)
    
    f_type = infer_type(fname)

    for i, para in enumerate(doc.paragraphs):
        line = para.text.strip()
        if not line: continue
        
        # å¿½ç•¥é¡µçœ‰é¡µè„šå™ªéŸ³
        if "é¢˜åº“" in line and "ç«èµ›" in line: continue
        if line.isdigit(): continue # å¿½ç•¥çº¯æ•°å­—é¡µç 

        # 1. ç­”æ¡ˆ
        ans_match = RE_ANSWER.match(line)
        if ans_match:
            if curr:
                curr['answer'] = clean_answer(ans_match.group(1))
                collecting_txt = False
            continue
            
        # 2. è§£æ
        ana_match = RE_ANALYSIS.match(line)
        if ana_match:
            if curr:
                parts = RE_CHAPTER_IN_ANA.split(ana_match.group(1))
                if len(parts) > 1:
                    curr['analysis'] = parts[0].strip()
                    curr['chapter'] = parts[1].replace("æ‰€åœ¨ç« èŠ‚ï¼š","").strip()
                else:
                    curr['analysis'] = parts[0].strip()
                collecting_txt = False
            continue

        # 3. é€‰é¡¹
        opt_match = RE_OPTION.match(line)
        if opt_match:
            if curr:
                if 'options' not in curr: curr['options'] = []
                curr['options'].append(line)
                collecting_txt = False
            continue

        # 4. æ–°é¢˜ç›®åˆ¤å®š
        is_new = False
        if curr is None: is_new = True
        elif 'answer' in curr and curr['answer']: 
            questions.append(curr)
            is_new = True
        
        if is_new:
            curr = {
                "question": line,
                "options": [],
                "answer": "",
                "analysis": "",
                "type": f_type,
                "source_file": fname,
                "line_no": i + 1 # è®°å½•è¡Œå·æ–¹ä¾¿æ’æŸ¥
            }
            collecting_txt = True
        elif collecting_txt and curr:
            curr['question'] += "<br>" + line

    if curr and 'answer' in curr:
        questions.append(curr)
    
    log(f"   ğŸ“„ {fname}: æ‰«æ {total_paragraphs} è¡Œ -> æå– {len(questions)} é¢˜")
    return questions

def quality_check(all_qs):
    """æ•°æ®è´¨é‡æ£€æŸ¥"""
    valid_qs = []
    invalid_qs = []
    
    for q in all_qs:
        reason = []
        # æ£€æŸ¥1: æ²¡æœ‰ç­”æ¡ˆ
        if not q['answer']:
            reason.append("ç¼ºå¤±ç­”æ¡ˆ")
        
        # æ£€æŸ¥2: å•/å¤šé€‰æ²¡æœ‰é€‰é¡¹
        if q['type'] in ['single', 'multi'] and len(q['options']) < 2:
            reason.append("é€‰é¡¹è¿‡å°‘")
            
        # æ£€æŸ¥3: é¢˜å¹²è¿‡çŸ­
        if len(normalize_text(q['question'])) < 3:
            reason.append("é¢˜å¹²è¿‡çŸ­(ç–‘ä¼¼å™ªéŸ³)")
            
        if reason:
            q['error_reason'] = ", ".join(reason)
            invalid_qs.append(q)
        else:
            valid_qs.append(q)
            
    return valid_qs, invalid_qs

def process_and_deduplicate(all_qs):
    db = {"single": [], "multi": [], "tf": []}
    seen = {} 
    duplicates_info = [] # è®°å½•é‡å¤è¯¦æƒ…
    
    for q in all_qs:
        # ç±»å‹å…œåº•
        if q['type'] == 'unknown':
            if q['answer'] in ['æ­£ç¡®', 'é”™è¯¯']: q['type'] = 'tf'
            elif len(q['answer']) > 1: q['type'] = 'multi'
            else: q['type'] = 'single'
            
        if q['type'] == 'tf' and not q['options']:
            q['options'] = ["A. æ­£ç¡®", "B. é”™è¯¯"]

        # æŒ‡çº¹
        fingerprint = normalize_text(q['question']) + q['answer']
        q_hash = hashlib.md5(fingerprint.encode('utf-8')).hexdigest()
        
        if q_hash in seen:
            old_q = seen[q_hash]
            # è®°å½•é‡å¤ä¿¡æ¯
            duplicates_info.append(f"ã€é‡å¤ã€‘{q['source_file']} (è¡Œ{q['line_no']}) ä¸ {old_q['source_file']} (è¡Œ{old_q['line_no']}) å†…å®¹ç›¸åŒã€‚ä¿ç•™è§£æè¾ƒé•¿è€…ã€‚")
            
            if len(q['analysis']) > len(old_q['analysis']):
                seen[q_hash] = q
        else:
            seen[q_hash] = q
            
    # å½’ç±»
    for q_hash, q in seen.items():
        q['id'] = q_hash 
        # æ¸…ç†ä¸´æ—¶å­—æ®µ
        del q['source_file']
        del q['line_no']
        
        if q['type'] in db:
            db[q['type']].append(q)
            
    return db, duplicates_info

if __name__ == "__main__":
    log("=== å¼€å§‹æ‰§è¡Œé¢˜åº“æå–ä¸éªŒè¯ç¨‹åº ===")
    
    if not os.path.exists(INPUT_DIR):
        log(f"âŒ æ‰¾ä¸åˆ° {INPUT_DIR} æ–‡ä»¶å¤¹")
    else:
        # 1. è¯»å–
        raw_data = []
        files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.docx') and not f.startswith('~$')]
        
        if not files:
            log(f"âš ï¸  {INPUT_DIR} ä¸­æ²¡æœ‰ .docx æ–‡ä»¶")
        else:
            for f in files:
                raw_data.extend(parse_docx(os.path.join(INPUT_DIR, f)))
            
            log(f"\nğŸ“¥ å…±æå–åŸå§‹é¢˜ç›®: {len(raw_data)} é“")
            
            # 2. è´¨é‡æ£€æŸ¥
            valid_data, invalid_data = quality_check(raw_data)
            
            if invalid_data:
                log(f"âš ï¸  å‘ç° {len(invalid_data)} é“å¼‚å¸¸é¢˜ç›® (å°†è¢«ä¸¢å¼ƒï¼Œè¯¦æƒ…è§æŠ¥å‘Šåº•ç«¯)")
            else:
                log("âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡ï¼Œæœªå‘ç°æ ¼å¼é”™è¯¯çš„é¢˜ç›®ã€‚")

            # 3. å»é‡
            final_db, dup_info = process_and_deduplicate(valid_data)
            
            # 4. ç»Ÿè®¡
            count_single = len(final_db['single'])
            count_multi = len(final_db['multi'])
            count_tf = len(final_db['tf'])
            total_final = count_single + count_multi + count_tf
            
            log(f"\nğŸ§¹ å»é‡åæœ‰æ•ˆé¢˜ç›®: {total_final} é“")
            log(f"   â”œâ”€ å•é€‰é¢˜: {count_single}")
            log(f"   â”œâ”€ å¤šé€‰é¢˜: {count_multi}")
            log(f"   â””â”€ åˆ¤æ–­é¢˜: {count_tf}")
            
            # 5. è¾“å‡º JS
            js_content = f"const QUESTION_DB = {json.dumps(final_db, ensure_ascii=False, indent=2)};"
            with open(OUTPUT_JS, 'w', encoding='utf-8') as f:
                f.write(js_content)
            log(f"\nğŸ’¾ é¢˜åº“æ–‡ä»¶å·²ç”Ÿæˆ: {OUTPUT_JS}")
            
            # 6. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            with open(REPORT_FILE, 'w', encoding='utf-8') as f:
                f.write("\n".join(log_buffer))
                f.write("\n\n" + "="*30 + " é‡å¤é¢˜ç›®æ˜ç»† " + "="*30 + "\n")
                if dup_info:
                    f.write("\n".join(dup_info))
                else:
                    f.write("æ— é‡å¤é¢˜ç›®ã€‚")
                
                f.write("\n\n" + "="*30 + " å¼‚å¸¸/ä¸¢å¼ƒé¢˜ç›®æ˜ç»† " + "="*30 + "\n")
                if invalid_data:
                    for q in invalid_data:
                        f.write(f"âŒ [{q['error_reason']}] {q['source_file']} (è¡Œ{q['line_no']}): {q['question'][:30]}...\n")
                else:
                    f.write("æ— å¼‚å¸¸é¢˜ç›®ã€‚")
                    
            print(f"ğŸ“‹ è¯¦ç»†éªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ: {os.path.abspath(REPORT_FILE)}")
            print("ğŸ‘‰ è¯·æ‰“å¼€æŠ¥å‘ŠæŸ¥çœ‹æ˜¯å¦æœ‰é¢˜ç›®é—æ¼ï¼")